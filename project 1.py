# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pt_1Rw0szEZhIv7tHFitniYSqiENpLw9
"""

import pandas as pd

df=pd.read_csv('/content/bankmarketing.csv')

"""# Task
Perform an exploratory data analysis (EDA) on the `df` DataFrame to understand its structure, distributions, and identify any patterns or issues, and then summarize the key findings.

## Load Data and Initial Inspection

### Subtask:
Display the first few rows of the DataFrame, check its dimensions, and examine the data types of each column to understand the dataset's structure.

**Reasoning**:
To understand the dataset's structure, I will display the first few rows, check its dimensions, and examine the data types of each column as requested in the subtask.
"""

print("First 5 rows of the DataFrame:")
print(df.head())

print("\nDimensions of the DataFrame (rows, columns):")
print(df.shape)

print("\nData types of each column:")
df.info()

"""## Summary Statistics

### Subtask:
Generate descriptive statistics for numerical columns to understand their central tendency, dispersion, and shape. Also, identify unique values and their counts for categorical columns.

**Reasoning**:
To address the subtask, I will first generate descriptive statistics for all numerical columns using `df.describe()`, and then iterate through the DataFrame's columns to identify and print unique values and their counts for categorical columns (object type).
"""

print("\nDescriptive statistics for numerical columns:")
print(df.describe())

print("\nUnique values and counts for categorical columns:")
for column in df.columns:
    if df[column].dtype == 'object':
        print(f"\nColumn: {column}")
        print(df[column].value_counts())

"""## Check for Missing Values

### Subtask:
Identify and quantify any missing values in the dataset to determine if imputation or removal is necessary.

**Reasoning**:
To identify and quantify missing values, I will calculate the sum of null values for each column and then compute the percentage of missing values for each column.
"""

missing_values = df.isnull().sum()
missing_percentage = (df.isnull().sum() / len(df)) * 100

missing_df = pd.DataFrame({'Missing Count': missing_values, 'Missing Percentage': missing_percentage})

print("Missing Values Report:")
print(missing_df[missing_df['Missing Count'] > 0])

if missing_df[missing_df['Missing Count'] > 0].empty:
    print("No missing values found in the dataset.")

"""## Visualize Data Distributions

### Subtask:
Create various plots (e.g., histograms for numerical features, bar plots for categorical features) to visualize the distribution of key variables. Include legends where appropriate.

**Reasoning**:
To visualize the distributions as requested, I will import `matplotlib.pyplot` and `seaborn`, then iterate through numerical columns to generate histograms and through categorical columns to create bar plots, setting appropriate labels and titles for each.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Get numerical and categorical columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

print("Visualizing Distributions for Numerical Columns:")
for col in numerical_cols:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col.replace("_", " ").title()}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(axis='y', alpha=0.75)
    plt.show()

print("\nVisualizing Distributions for Categorical Columns:")
for col in categorical_cols:
    plt.figure(figsize=(10, 6))
    sns.countplot(y=df[col], order=df[col].value_counts().index)
    plt.title(f'Distribution of {col.replace("_", " ").title()}')
    plt.xlabel('Count')
    plt.ylabel(col)
    plt.grid(axis='x', alpha=0.75)
    plt.tight_layout()
    plt.show()

"""## Analyze Target Variable Distribution

### Subtask:
Examine the distribution of the target variable 'y' (subscription to a term deposit) to understand its balance and potential impact on model training. Provide a plot with a legend.

**Reasoning**:
To examine the distribution of the target variable 'y' and visualize its balance, I will create a count plot using `seaborn.countplot()` as requested in the instructions.
"""

plt.figure(figsize=(8, 6))
sns.countplot(x='y', data=df, palette='viridis')
plt.title('Distribution of Target Variable (y)')
plt.xlabel('Subscription Status')
plt.ylabel('Count')
plt.grid(axis='y', alpha=0.75)
plt.show()

"""**Reasoning**:
The previous code block generated a `FutureWarning` regarding the usage of `palette` without `hue` in `seaborn.countplot`. To address this, I will explicitly set `hue='y'` and `legend=False` in the `countplot` function call.


"""

plt.figure(figsize=(8, 6))
sns.countplot(x='y', hue='y', data=df, palette='viridis', legend=False)
plt.title('Distribution of Target Variable (y)')
plt.xlabel('Subscription Status')
plt.ylabel('Count')
plt.grid(axis='y', alpha=0.75)
plt.show()

"""## Final Task

### Subtask:
Summarize the key findings from the exploratory data analysis, highlighting any significant patterns, outliers, or issues identified.

## Summary:

### Data Analysis Key Findings

*   The dataset `df` contains 41,188 rows and 21 columns, comprising 5 integer, 11 object (categorical), and 5 float columns.
*   No missing values were found across any columns in the dataset.
*   The `duration` column, representing call duration, has a minimum value of 0 seconds, which might indicate immediately disconnected calls or invalid entries.
*   The `pdays` column, indicating days since the last contact, shows that the vast majority of clients (75th percentile at 999) were not previously contacted, as 999 signifies no prior contact.
*   The `default` column has a significant number of 'unknown' entries (8,597), with only 3 instances of 'yes', suggesting potential data quality issues or a very small number of defaulters.
*   The target variable, `y` (subscription to a term deposit), is highly imbalanced, with 'no' responses (36,548) vastly outnumbering 'yes' responses (4,640).
*   Categorical columns like `job`, `marital`, and `education` also contain 'unknown' categories, requiring further investigation.
*   'admin.', 'blue-collar', and 'technician' are the most frequent job types. 'married' is the most common marital status. 'university.degree' and 'high.school' are predominant education levels.
*   'may' is the most frequent contact month, and 'cellular' is the more common contact method.
*   The outcome of the previous marketing campaign (`poutcome`) was overwhelmingly 'nonexistent'.

### Insights or Next Steps

*   **Address Class Imbalance:** The severe class imbalance in the target variable `y` (subscription status) needs to be addressed before model training, possibly through techniques like oversampling the minority class, undersampling the majority class, or using algorithms robust to imbalance.
*   **Investigate 'Unknown' Values and Anomalies:** Further investigation is needed for 'unknown' entries in categorical columns (`default`, `job`, `marital`, `education`) to determine if they represent missing data, a specific category, or should be imputed. Additionally, explore the `duration` entries with a value of 0 to understand their implications.
"""